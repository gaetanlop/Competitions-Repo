{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01042,
     "end_time": "2021-03-01T07:11:29.119985",
     "exception": false,
     "start_time": "2021-03-01T07:11:29.109565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Goal of the notebook\n",
    "\n",
    "I decided to participate to this competition in order to increase my understanding of gradient boosting algorithms. I found the notebook of **Awwal Malhi** (https://www.kaggle.com/awwalmalhi/extreme-fine-tuning-lgbm-using-7-step-training) about Extreme Gradient Boosting very interesting (special thanks to him) and I decided to take a deep dive into it. My notebook is just an implementation of the strategy that he used with some explanations and some improvements.\n",
    "\n",
    "\n",
    "## Pretrained LGBM strategy:\n",
    "\n",
    "This strategy enables me to go from **0.84198** to **0.84184** with a single lgbm model. **Awwal Malhi** already gave some explanations of his strategy, but I will try to add mine.\n",
    "\n",
    "### How it works ?\n",
    "\n",
    "* train your best model\n",
    "* decrease learning rate and train the model again\n",
    "* decrease regularization params and retrain the model\n",
    "\n",
    "### Explanations\n",
    "\n",
    "This strategy is mostly based on **transfer learning** (mostly used in neural networks). In transfer learning,we use a pretrained model and add a head to it. Moreover, we usually freeze lower layers (the ones of the pretrained model) and train higher layers (those that we add to the pretrained model). This is exactly the case here:\n",
    "\n",
    "We create a normal lgbm model and fit it on our data. Once it starts overfitting we stop the training. We will consider this part of the lgbm model as the pretrained model (to make an analogy to neural networks). \n",
    "\n",
    "After that, and in order to fight against overfitting, we decrease learning rate and starts fitting again the pretrained model on our data, in other words we add more weak learners to our pretrained model (that can be compared to higher layers in a neural network). We can also make an analogy to neural networks in this case. Indeed, when we train neural networks, it is good practice to decrease the learning rate during training process.\n",
    "\n",
    "Once reducing the learning rate is not adding a significant improvement to our model, we should increase the complexity of our weak learners. Indeed, increasing weak learners complexity might increase their performance while also increasing their chance of overfitting. At inference time, we will have weak learners with high bias and low variance (weak learners from the pretrained model) and some which are slightly overfit (low bias- high variance). This is why we reduce the learning rate before adding overfitted weak learning (when we reduce learning rate, we basically reduce the contribution of these overfitted trees to final prediction).\n",
    "\n",
    "I tried many things in order to increase model compelxity and decrease regularization params. I found that the best thing to do is to increase number of leaves and decrease minimum child samples. \n",
    "\n",
    "**This explanation was simply my understanding of how this strategy works. If you think anything is wrong or you want to add something, feel free to add a comment, I will be glad to read it and find what others think of how it works.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009261,
     "end_time": "2021-03-01T07:11:29.138922",
     "exception": false,
     "start_time": "2021-03-01T07:11:29.129661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-01T07:11:29.160830Z",
     "iopub.status.busy": "2021-03-01T07:11:29.160208Z",
     "iopub.status.idle": "2021-03-01T07:11:31.244821Z",
     "shell.execute_reply": "2021-03-01T07:11:31.244148Z"
    },
    "papermill": {
     "duration": 2.096912,
     "end_time": "2021-03-01T07:11:31.245124",
     "exception": false,
     "start_time": "2021-03-01T07:11:29.148212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T07:11:31.271231Z",
     "iopub.status.busy": "2021-03-01T07:11:31.270575Z",
     "iopub.status.idle": "2021-03-01T07:11:35.296042Z",
     "shell.execute_reply": "2021-03-01T07:11:35.295010Z"
    },
    "papermill": {
     "duration": 4.040239,
     "end_time": "2021-03-01T07:11:35.296213",
     "exception": false,
     "start_time": "2021-03-01T07:11:31.255974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('../input/tabular-playground-series-feb-2021/train.csv')\n",
    "test=pd.read_csv('../input/tabular-playground-series-feb-2021/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010115,
     "end_time": "2021-03-01T07:11:35.316815",
     "exception": false,
     "start_time": "2021-03-01T07:11:35.306700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T07:11:35.383711Z",
     "iopub.status.busy": "2021-03-01T07:11:35.378557Z",
     "iopub.status.idle": "2021-03-01T07:11:37.198421Z",
     "shell.execute_reply": "2021-03-01T07:11:37.198883Z"
    },
    "papermill": {
     "duration": 1.871792,
     "end_time": "2021-03-01T07:11:37.199091",
     "exception": false,
     "start_time": "2021-03-01T07:11:35.327299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_var=[f'cat{i}' for i in range(10)]\n",
    "cont_var=[f'cont{i}' for i in range(14)]\n",
    "columns=[ col for col in train.columns.tolist() if col not in ['id','target']]\n",
    "\n",
    "for cat in cat_var:\n",
    "    le = LabelEncoder()\n",
    "    train[cat]=le.fit_transform(train[cat])\n",
    "    test[cat]=le.transform(test[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T07:11:37.225837Z",
     "iopub.status.busy": "2021-03-01T07:11:37.225092Z",
     "iopub.status.idle": "2021-03-01T07:11:37.270549Z",
     "shell.execute_reply": "2021-03-01T07:11:37.269961Z"
    },
    "papermill": {
     "duration": 0.061142,
     "end_time": "2021-03-01T07:11:37.270706",
     "exception": false,
     "start_time": "2021-03-01T07:11:37.209564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X=train[columns]\n",
    "y=train.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010457,
     "end_time": "2021-03-01T07:11:37.291789",
     "exception": false,
     "start_time": "2021-03-01T07:11:37.281332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010594,
     "end_time": "2021-03-01T07:11:37.313375",
     "exception": false,
     "start_time": "2021-03-01T07:11:37.302781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I wanted to say many thanks to **Hamza** (https://www.kaggle.com/hamzaghanmi/lgbm-hyperparameter-tuning-using-optuna) who creates an amazing notebook for tuning hyperparameters using Optuna, I learnt a lot from it. Using Optuna and a 5 fold cross validation strategy,the best result I could get is 0.84201 on public lb. So for my experiments I decided to use the hyperparameters of **Bizen** (https://www.kaggle.com/hiro5299834/tps-feb-2021-with-single-lgbm-tuned) which gave him a slightly better score of 0.84198 on public lb. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010749,
     "end_time": "2021-03-01T07:11:37.335030",
     "exception": false,
     "start_time": "2021-03-01T07:11:37.324281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def objective(trial,train=train,target=y):\n",
    "    \n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "    \n",
    "    param={\n",
    "        'num_leaves':trial.suggest_int('num_leaves',100,1000),\n",
    "        'max_depth':trial.suggest_categorical('max_depth',[7,10,20,50]),\n",
    "        'min_child_samples':trial.suggest_int('min_child_samples',100,500),\n",
    "        'max_bin':trial.suggest_categorical('max_bin',[255,350,512,1024]),\n",
    "        'learning_rate':trial.suggest_categorical('learning_rate',[0.006,0.008,0.01,0.014,0.017,0.02]),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "        'metric': 'rmse', \n",
    "        'random_state': 48,\n",
    "        'n_estimators': 30000\n",
    "         \n",
    "    }\n",
    "    \n",
    "    model=LGBMRegressor(**param)\n",
    "    \n",
    "    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n",
    "    \n",
    "    predictions=model.predict(X_test)\n",
    "    \n",
    "    rmse=mean_squared_error(y_test,predictions,squared=False)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T07:11:37.363150Z",
     "iopub.status.busy": "2021-03-01T07:11:37.362466Z",
     "iopub.status.idle": "2021-03-01T07:11:37.364931Z",
     "shell.execute_reply": "2021-03-01T07:11:37.364317Z"
    },
    "papermill": {
     "duration": 0.019495,
     "end_time": "2021-03-01T07:11:37.365071",
     "exception": false,
     "start_time": "2021-03-01T07:11:37.345576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# base lgbm models\n",
    "\n",
    "\n",
    "lgb_params={'random_state': 2021,\n",
    "          'metric': 'rmse',\n",
    "          'n_estimators': 30000,\n",
    "          'n_jobs': -1,\n",
    "          'cat_feature': [x for x in range(len(cat_var))],\n",
    "          'bagging_seed': 2021,\n",
    "          'feature_fraction_seed': 2021,\n",
    "          'learning_rate': 0.003899156646724397,\n",
    "          'max_depth': 99,\n",
    "          'num_leaves': 63,\n",
    "          'reg_alpha': 9.562925363678952,\n",
    "          'reg_lambda': 9.355810045480153,\n",
    "          'colsample_bytree': 0.2256038826485174,\n",
    "          'min_child_samples': 290,\n",
    "          'subsample_freq': 1,\n",
    "          'subsample': 0.8805303688019942,\n",
    "          'max_bin': 882,\n",
    "          'min_data_per_group': 127,\n",
    "          'cat_smooth': 96,\n",
    "          'cat_l2': 19\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010198,
     "end_time": "2021-03-01T07:11:37.386326",
     "exception": false,
     "start_time": "2021-03-01T07:11:37.376128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pretrained lgbm model strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T07:11:37.414128Z",
     "iopub.status.busy": "2021-03-01T07:11:37.413438Z",
     "iopub.status.idle": "2021-03-01T07:11:37.416399Z",
     "shell.execute_reply": "2021-03-01T07:11:37.415781Z"
    },
    "papermill": {
     "duration": 0.018488,
     "end_time": "2021-03-01T07:11:37.416540",
     "exception": false,
     "start_time": "2021-03-01T07:11:37.398052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# factor by which we will reduce lambda\n",
    "f1= 0.6547870667136243\n",
    "# we will increase alpha by 2.6 each time we retrain the model\n",
    "f2= 2.6711351556035487\n",
    "# increase number of leaves by 20 each time we retrain the model\n",
    "f3= 20\n",
    "# decrease min child samples by 49 each time we retrain the model\n",
    "f4= 49\n",
    "\n",
    "f5= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T07:11:37.449832Z",
     "iopub.status.busy": "2021-03-01T07:11:37.449084Z",
     "iopub.status.idle": "2021-03-01T13:51:23.392569Z",
     "shell.execute_reply": "2021-03-01T13:51:23.393119Z"
    },
    "papermill": {
     "duration": 23985.965736,
     "end_time": "2021-03-01T13:51:23.393316",
     "exception": false,
     "start_time": "2021-03-01T07:11:37.427580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Round:\n",
      "RMSE 0.8417286531304851\n",
      "RMSE tuned 1: 0.8417286979346285\n",
      "RMSE tuned 2: 0.8417287480497231\n",
      "RMSE tuned 3: 0.841726938816642\n",
      "RMSE tuned 4: 0.8417221230010475\n",
      "RMSE tuned 5: 0.8417215805130301\n",
      "RMSE tuned 6: 0.8417208759546769\n",
      "RMSE tuned 7: 0.8417200664126364\n",
      "RMSE tuned 8: 0.8417195340452981\n",
      "RMSE tuned 9: 0.8417187093752144\n",
      "RMSE tuned 10: 0.8417174869320329\n",
      "RMSE tuned 11: 0.8417161499132937\n",
      "RMSE tuned 12: 0.8417160046945139\n",
      "RMSE tuned 13: 0.8417159629925569\n",
      "RMSE tuned 14: 0.8417160035464603\n",
      "RMSE tuned 15: 0.841716019394132\n",
      "RMSE tuned 16: 0.8417160238842415\n",
      "Improvement of 1.2629246243567316e-05\n",
      "First Round:\n",
      "RMSE 0.8451388043495696\n",
      "RMSE tuned 1: 0.8451342007426411\n",
      "RMSE tuned 2: 0.8451307966128719\n",
      "RMSE tuned 3: 0.8451240854976896\n",
      "RMSE tuned 4: 0.8451204440825001\n",
      "RMSE tuned 5: 0.8451173649176033\n",
      "RMSE tuned 6: 0.8451169403211635\n",
      "RMSE tuned 7: 0.8451149064330081\n",
      "RMSE tuned 8: 0.845114177059101\n",
      "RMSE tuned 9: 0.8451130577896553\n",
      "RMSE tuned 10: 0.8451121965684848\n",
      "RMSE tuned 11: 0.8451104558629394\n",
      "RMSE tuned 12: 0.8451099023796963\n",
      "RMSE tuned 13: 0.8451092021376316\n",
      "RMSE tuned 14: 0.8451088298470804\n",
      "RMSE tuned 15: 0.8451082224102294\n",
      "RMSE tuned 16: 0.8451080855087455\n",
      "Improvement of 3.071884082406218e-05\n",
      "First Round:\n",
      "RMSE 0.8428975542600424\n",
      "RMSE tuned 1: 0.8428799054229363\n",
      "RMSE tuned 2: 0.8428686720031063\n",
      "RMSE tuned 3: 0.8428430888603566\n",
      "RMSE tuned 4: 0.842835646812286\n",
      "RMSE tuned 5: 0.8428275591900294\n",
      "RMSE tuned 6: 0.842827060111216\n",
      "RMSE tuned 7: 0.8428167130675689\n",
      "RMSE tuned 8: 0.8428162237821142\n",
      "RMSE tuned 9: 0.8428162606394006\n",
      "RMSE tuned 10: 0.8428151820810516\n",
      "RMSE tuned 11: 0.8428151282767941\n",
      "RMSE tuned 12: 0.8428139512615992\n",
      "RMSE tuned 13: 0.8428129605661441\n",
      "RMSE tuned 14: 0.8428107918672854\n",
      "RMSE tuned 15: 0.8428089885309615\n",
      "RMSE tuned 16: 0.84280666884874\n",
      "Improvement of 9.088541130242156e-05\n",
      "First Round:\n",
      "RMSE 0.8390687533454567\n",
      "RMSE tuned 1: 0.8390658971740552\n",
      "RMSE tuned 2: 0.8390641194919143\n",
      "RMSE tuned 3: 0.8390622248264483\n",
      "RMSE tuned 4: 0.8390600323283152\n",
      "RMSE tuned 5: 0.8390560463448894\n",
      "RMSE tuned 6: 0.8390526593139235\n",
      "RMSE tuned 7: 0.8390476533435652\n",
      "RMSE tuned 8: 0.8390438429870424\n",
      "RMSE tuned 9: 0.8390384072794951\n",
      "RMSE tuned 10: 0.8390328888803092\n",
      "RMSE tuned 11: 0.8390314823318294\n",
      "RMSE tuned 12: 0.8390305465245314\n",
      "RMSE tuned 13: 0.8390292129175463\n",
      "RMSE tuned 14: 0.839028457937729\n",
      "RMSE tuned 15: 0.8390273676522231\n",
      "RMSE tuned 16: 0.8390260824069561\n",
      "Improvement of 4.267093850063297e-05\n",
      "First Round:\n",
      "RMSE 0.8403094756119941\n",
      "RMSE tuned 1: 0.8403042810122184\n",
      "RMSE tuned 2: 0.8403030179484398\n",
      "RMSE tuned 3: 0.8402985439048303\n",
      "RMSE tuned 4: 0.8402982295850594\n",
      "RMSE tuned 5: 0.8402975325095783\n",
      "RMSE tuned 6: 0.8402946225790305\n",
      "RMSE tuned 7: 0.840290530874043\n",
      "RMSE tuned 8: 0.8402873546838611\n",
      "RMSE tuned 9: 0.8402867249084989\n",
      "RMSE tuned 10: 0.8402864205781726\n",
      "RMSE tuned 11: 0.8402860157592643\n",
      "RMSE tuned 12: 0.840285113841945\n",
      "RMSE tuned 13: 0.8402850324747898\n",
      "RMSE tuned 14: 0.84028469527142\n",
      "RMSE tuned 15: 0.8402845741545457\n",
      "RMSE tuned 16: 0.8402844874416955\n",
      "Improvement of 2.498817029861211e-05\n",
      "CPU times: user 1d 2h 1min 43s, sys: 1min 51s, total: 1d 2h 3min 34s\n",
      "Wall time: 6h 39min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "kf=KFold(n_splits=5,random_state=48,shuffle=True)\n",
    "\n",
    "# we will store our final predictions in preds\n",
    "preds = np.zeros(test.shape[0])\n",
    "#store rmse of each iterations\n",
    "rmse=[]\n",
    "i=0\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Phase 1: create the pretrained model\n",
    "for idx_train,idx_test in kf.split(X,y):\n",
    "    \n",
    "    X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n",
    "    y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n",
    "\n",
    "    \n",
    "    model=LGBMRegressor(**lgb_params)\n",
    "    \n",
    "    model.fit(X_train,y_train,eval_set=(X_test,y_test),early_stopping_rounds=300,verbose=False,eval_metric='rmse')\n",
    "    \n",
    "    predictions=model.predict(X_test,num_iteration=model.best_iteration_)\n",
    "    \n",
    "    rmse.append(mean_squared_error(y_test,predictions,squared=False))\n",
    "    \n",
    "    print('First Round:')\n",
    "    \n",
    "    print(f'RMSE {rmse[i]}')\n",
    "    \n",
    "    rmse_tuned=[]\n",
    "    params = lgb_params.copy()\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Phase 2: iterations where we decrease the learning rate and regularization params    \n",
    "    for t in range(1,17):\n",
    "        \n",
    "        \n",
    "        if t >2:    \n",
    "                    \n",
    "            params['reg_lambda'] *=  f1\n",
    "            params['reg_alpha'] += f2\n",
    "            params['num_leaves'] += f3\n",
    "            params['min_child_samples'] -= f4\n",
    "            params['cat_smooth'] -= f5\n",
    "        \n",
    "            \n",
    "        params['learning_rate']=0.003\n",
    "        \n",
    "        # min_child_samples can not be lower than 0\n",
    "        if params['min_child_samples']<1:\n",
    "            params['min_child_samples']=1\n",
    "        \n",
    "        # we decrease the learning rate even more after 11 rounds of retraining\n",
    "        if t>11:\n",
    "            params['learning_rate']=0.001\n",
    "              \n",
    "        \n",
    "        model=LGBMRegressor(**params).fit(X_train,y_train,eval_set=(X_test,y_test),eval_metric='rmse',early_stopping_rounds=200,verbose=False,init_model=model)\n",
    "        \n",
    "        predictions=model.predict(X_test, num_iteration= model.best_iteration_)\n",
    "        \n",
    "        rmse_tuned.append(mean_squared_error(y_test,predictions,squared=False))\n",
    "        \n",
    "        print(f'RMSE tuned {t}: {rmse_tuned[t-1]}')\n",
    "        \n",
    "    print(f'Improvement of {rmse[i]-rmse_tuned[t-1]}')\n",
    "    \n",
    "    # ---------------------------------------------------------------------------\n",
    "    # Inference time: calculate predictions for test set\n",
    "    \n",
    "    preds+=model.predict(test[columns],num_iteration=model.best_iteration_)/kf.n_splits\n",
    "        \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T13:51:23.469542Z",
     "iopub.status.busy": "2021-03-01T13:51:23.468889Z",
     "iopub.status.idle": "2021-03-01T13:51:24.208090Z",
     "shell.execute_reply": "2021-03-01T13:51:24.207362Z"
    },
    "papermill": {
     "duration": 0.778255,
     "end_time": "2021-03-01T13:51:24.208244",
     "exception": false,
     "start_time": "2021-03-01T13:51:23.429989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "test['target']=preds\n",
    "test=test[['id','target']]\n",
    "test.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03638,
     "end_time": "2021-03-01T13:51:24.282147",
     "exception": false,
     "start_time": "2021-03-01T13:51:24.245767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Thanks for reading\n",
    "\n",
    "If you like this work and find it useful, upvote please."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24001.825307,
   "end_time": "2021-03-01T13:51:25.132192",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-01T07:11:23.306885",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
